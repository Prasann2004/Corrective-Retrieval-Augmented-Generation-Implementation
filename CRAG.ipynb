{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This code imports necessary modules and defines API keys for Gemini and Hugging Face.\n",
    "\"\"\"\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader, TextLoader, PyPDFLoader\n",
    "from langchain_community.embeddings import GPT4AllEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from googlesearch import search\n",
    "import google.generativeai as genai\n",
    "\n",
    "gemini_api_key = \"Paste-Your-API-Key-Here\"\n",
    "hf_key=\"Paste-Your-Hugging-Face-API-Key-Here\"\n",
    "Internal_knowledge_base = \"Enetr your knowledge base here\"#must be a pdf file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#safety_settings for gemini model\n",
    "safety_settings = [\n",
    "    {\n",
    "        \"category\": \"HARM_CATEGORY_DANGEROUS\",\n",
    "        \"threshold\": \"BLOCK_NONE\",\n",
    "    },\n",
    "    {\n",
    "        \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
    "        \"threshold\": \"BLOCK_NONE\",\n",
    "    },\n",
    "    {\n",
    "        \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
    "        \"threshold\": \"BLOCK_NONE\",\n",
    "    },\n",
    "    {\n",
    "        \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
    "        \"threshold\": \"BLOCK_NONE\",\n",
    "    },\n",
    "    {\n",
    "        \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
    "        \"threshold\": \"BLOCK_NONE\",\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieve Doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doc(question):\n",
    "    \"\"\"\n",
    "    Retrieves the relevant document page content based on the given question.\n",
    "\n",
    "    Args:\n",
    "        question (str): The question to search for in the document.\n",
    "\n",
    "    Returns:\n",
    "        str: The page content of the most relevant document.\n",
    "    \"\"\"\n",
    "    pdf = Internal_knowledge_base\n",
    "    loader = PyPDFLoader(pdf)\n",
    "    docs = loader.load()\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size=1000, chunk_overlap=50)\n",
    "    all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "    embegging = GPT4AllEmbeddings()\n",
    "\n",
    "    vectorstore = Chroma.from_documents(documents=all_splits, collection_name=\"rag-chroma\", embedding=embegging)\n",
    "    retriver = vectorstore.as_retriever()\n",
    "    docs = retriver.get_relevant_documents(question, k=1)\n",
    "    return docs[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt_retriever(context, question):\n",
    "    \"\"\"\n",
    "    Returns a prompt for a grader assessing the relevance of a retrieved document to a user question.\n",
    "\n",
    "    Parameters:\n",
    "    context (str): The retrieved document.\n",
    "    question (str): The user question.\n",
    "\n",
    "    Returns:\n",
    "    str: The prompt for the grader, including the retrieved document, user question, and instructions for grading.\n",
    "    \"\"\"\n",
    "    return f'''You are grader assessing relavance of a retrieved document to a user question. \\n\n",
    "    Here is the retrieved document:\\n\\n {context} \\n\n",
    "    Here is the user question:\\n\\n {question} \\n\n",
    "    If the document document contains keywords related to the user question, grade it as relevant. \\n\n",
    "    It does not need to be a stringent test.The goal is to filter out erroneous retrievels. \\n\n",
    "    Give a  score between 0 and 1 score to indicate the document is relevant to the question. \\n\n",
    "    Provide the score without any premable or explaination. \\n'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score(docs, question):\n",
    "    \"\"\"\n",
    "    Calculates the score for a given question based on the provided documents.\n",
    "\n",
    "    Parameters:\n",
    "    - docs (list): A list of documents to consider for generating the score.\n",
    "    - question (str): The question for which the score needs to be calculated.\n",
    "\n",
    "    Returns:\n",
    "    - float: The score for the given question.\n",
    "    \"\"\"\n",
    "    genai.configure(api_key=gemini_api_key)\n",
    "    model = genai.GenerativeModel('gemini-pro')\n",
    "    response = model.generate_content(get_prompt_retriever(docs, question), safety_settings=safety_settings)\n",
    "    return float(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Rewriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt_rewriter(question):\n",
    "    \"\"\"\n",
    "    Returns a rewritten prompt for a given question.\n",
    "\n",
    "    Parameters:\n",
    "    question (str): The original user question.\n",
    "\n",
    "    Returns:\n",
    "    str: The rewritten prompt.\n",
    "\n",
    "    \"\"\"\n",
    "    return f'''You are a question rewriter. \\n\n",
    "    Here is the user question:\\n\\n {question} \\n\n",
    "    Rewrite the question to make it more clear and concise. \\n\n",
    "    At the same time, try to keep the meaning of the question the same. \\n\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewrite_question(question):\n",
    "    \"\"\"\n",
    "    Rewrites the given question using the Gemini API.\n",
    "\n",
    "    Args:\n",
    "        question (str): The original question to be rewritten.\n",
    "\n",
    "    Returns:\n",
    "        str: The rewritten question generated by the Gemini model.\n",
    "    \"\"\"\n",
    "    genai.configure(api_key=gemini_api_key)\n",
    "    model = genai.GenerativeModel('gemini-pro')\n",
    "    response = model.generate_content(get_prompt_rewriter(question), safety_settings=safety_settings)\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Knowledge Refinement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def refine_doc(doc, question):\n",
    "    \"\"\"\n",
    "    Refines the given document by splitting it into smaller chunks, embedding them, and retrieving the most relevant documents based on a given question.\n",
    "\n",
    "    Args:\n",
    "        doc (str): The document to be refined.\n",
    "        question (str): The question to find relevant documents for.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the indices of the best documents and the best documents themselves.\n",
    "    \"\"\"\n",
    "    file = open('docs_to_refine.md', 'w', encoding=\"utf-8\") \n",
    "    file.write(doc) \n",
    "    file.close()\n",
    "    loader = TextLoader('docs_to_refine.md', encoding='UTF-8')\n",
    "    docs_to_refine = loader.load()\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size=250, chunk_overlap=50)\n",
    "    all_splits = text_splitter.split_documents(docs_to_refine)\n",
    "\n",
    "    embegging = GPT4AllEmbeddings()\n",
    "\n",
    "    vectorstore = Chroma.from_documents(documents=all_splits, collection_name=\"rag-chroma\", embedding=embegging)\n",
    "    retriver = vectorstore.as_retriever()\n",
    "\n",
    "    docs_refined = retriver.get_relevant_documents(question, k=1)\n",
    "\n",
    "    score = []\n",
    "    for i in docs_refined:\n",
    "        score.append(get_score(i.page_content, question))\n",
    "    best_doc_index = sorted(range(len(score)), key=lambda i: score[i])[-2:]\n",
    "    best_doc = [docs_refined[i] for i in best_doc_index]\n",
    "    return best_doc_index, best_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Knowledge Searching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def web_search(query, num_results=5):\n",
    "    \"\"\"\n",
    "    Perform a web search using the specified query and return a list of results.\n",
    "\n",
    "    Args:\n",
    "        query (str): The search query.\n",
    "        num_results (int, optional): The number of search results to retrieve. Defaults to 5.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of search results.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for result in search(query, num_results=num_results):\n",
    "        results.append(result)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def External_Knowledge(question):\n",
    "    \"\"\"\n",
    "    Retrieves external knowledge related to the given question.\n",
    "\n",
    "    Args:\n",
    "        question (str): The question to search for external knowledge.\n",
    "\n",
    "    Returns:\n",
    "        str: The page content of the most relevant document found.\n",
    "    \"\"\"\n",
    "    url= web_search(question)[0]\n",
    "    loader=WebBaseLoader(url)\n",
    "    docs=loader.load()\n",
    "\n",
    "    text_splitter=RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size=500,chunk_overlap=50)\n",
    "    all_splits_=text_splitter.split_documents(docs)\n",
    "\n",
    "\n",
    "    embedding=GPT4AllEmbeddings()\n",
    "\n",
    "    vectorstore_=Chroma.from_documents(documents=all_splits_,collection_name=\"rag-chroma\",embedding=embedding)\n",
    "    retriver_=vectorstore_.as_retriever()\n",
    "\n",
    "    docs=retriver_.get_relevant_documents(question,k=1)\n",
    "    return docs[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CRAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.3=>x --> not Incorrect\n",
    "\n",
    "0.3< x < 0.7 --> Ambiguous\n",
    "\n",
    "0.7>= x --> Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CRAG(question):\n",
    "    \"\"\"\n",
    "    Retrieves relevant documents based on the given question and returns the content of the documents.\n",
    "\n",
    "    Args:\n",
    "        question (str): The question to be answered.\n",
    "\n",
    "    Returns:\n",
    "        str: The content of the relevant documents.\n",
    "\n",
    "    \"\"\"\n",
    "    docs=get_doc(question)\n",
    "    score=get_score(docs,question)\n",
    "    if score >=0.7 :\n",
    "        score_refined_doc,refined_doc=refine_doc(docs,question)\n",
    "        return refined_doc[0].page_content + '\\n\\n' + refined_doc[1].page_content\n",
    "    elif 0.3<score <0.7 :\n",
    "        score_refined_doc,refined_doc=refine_doc(docs,question)\n",
    "        external_knowledge=External_Knowledge(question)\n",
    "        return external_knowledge + '\\n\\n' + refined_doc[score_refined_doc[-1]].page_content\n",
    "    else:\n",
    "        external_knowledge=External_Knowledge(question)\n",
    "        return external_knowledge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question=\"What is the integration of x^2 ?\"\n",
    "CRAG(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference on Gemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt_inference(context, question):\n",
    "    \"\"\"\n",
    "    Generate a prompt for a question answering model.\n",
    "\n",
    "    Args:\n",
    "        context (str): The retrieved document.\n",
    "        question (str): The user question.\n",
    "\n",
    "    Returns:\n",
    "        str: The generated prompt for the model.\n",
    "    \"\"\"\n",
    "    return f'''You are a question answering model. \\n\n",
    "    Here is the retrieved document:\\n\\n {context} \\n\n",
    "    Here is the user question:\\n\\n {question} \\n\n",
    "    Answer the user question based on the retrieved document. \\n\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(hf_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "def generate_text(context, question):\n",
    "    \"\"\"\n",
    "    Generate text based on the given context and question using a pre-trained language model.\n",
    "\n",
    "    Args:\n",
    "        context (str): The context for generating the text.\n",
    "        question (str): The question to be answered in the generated text.\n",
    "\n",
    "    Returns:\n",
    "        str: The generated text.\n",
    "    \"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-7b-it\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\"google/gemma-7b-it\")\n",
    "\n",
    "    input_text = get_prompt_inference(context, question)\n",
    "    input_ids = tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "    outputs = model.generate(**input_ids)\n",
    "    return tokenizer.decode(outputs[0])\n",
    "\n",
    "generated_text = generate_text(context, question)\n",
    "print(generated_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
